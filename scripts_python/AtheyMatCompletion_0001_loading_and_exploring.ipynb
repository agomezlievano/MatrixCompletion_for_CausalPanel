{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "def ends(df, x=5):\n",
    "    return df.head(x).append(df.tail(x))\n",
    "setattr(pd.DataFrame,'ends',ends)\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What we're doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many years now, I've been very interested in two related, but different questions:\n",
    "1. How to _apply_ Machine Learning to Economics by integrating ML into the usual econometric pipeline?\n",
    "2. What do the ML algorithms themselves teach us about how economies work.\n",
    "\n",
    "Recently, however, there seems to be a wave of interest (especially coming from the work coming from Stanford University) in these questions. This notebook is about the point 1. In particular, about the problem of estimating causal inference. (Unfortunately, point 2 seems to be still far from being considered.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to replicate the methodology (and hopefully results) of a recent paper in which the problem of causal inference is seen from the lens of the problem of ''matrix completion'' from the machine learning literature. I found out about this paper thanks to my dear friend Sid Ravinutala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I became aware of this paper, I've become increasingly interested in **Susan Athey**'s work, who's precisely preaching about the importance of using ML in economics. Let's see what she did here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper we want to reproduce is: \n",
    "\n",
    "''Matrix Completion Methods for Causal Panel Data Models''\n",
    "by Susan Athey, Mohsen Bayati, Nikolay Doudchenko, Guido Imbens, Khashayar Khosravi\n",
    "(NBER Working Paper No. 25132, also available in [arxiv](https://arxiv.org/pdf/1710.10251.pdf))\n",
    "\n",
    "NOTE: Check [her GitHub](https://github.com/susanathey/), as she made available her code in R. I'll try to implement her functions in Python instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setup of the problem is to consider $N$ units across $T$ time steps. For each unit and time, there are two potential outcomes: $Y_{it}(0)$ if not treated, $Y_{it}(1)$ if treated. The treatment variable can be denoted by $W_{it}$. Hence, the realized outcome is $Y_{it}=Y_{it}(W_{it})$.\n",
    "\n",
    "The basic goal of causal identification is to estimate \n",
    "$$\\widehat{ATE} \\equiv \\frac{1}{NT}\\sum_{i,t}\\left(Y_{it}(1) - Y_{it}(0)\\right.)$$ \n",
    "The fundamental problem in causal identification, however, is that we never observe both $Y_{it}(0)$ and $Y_{it}(1)$. We either observe one, or the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Athey's et al. (2018) solution is based on the following: First, $Y_{it}(0)$ (or $Y_{it}(1)$) is a $N\\times T$ matrix with missing values. Second, there are several methodologies to impute missing values in matrices. Hence, to get the ''counterfactual'' matrix, we can make use of the methods to impute missing values.\n",
    "\n",
    "Athey et al. focus specifically on imputing missing values in $Y_{it}(0)$ (i.e., the counterfactuals of those treated).\n",
    "\n",
    "Many algorithms for imputing missing values in matrices are based on matrix factorization methods. The different methods differ in the constraints. See [Udell et al. (2015)](https://arxiv.org/pdf/1410.0342.pdf) for a good review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Athey's et al. (2018) use the method ''Nuclear Norm Matrix Completion Estimator''."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuclear Norm Matrix Completion Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us represent the $N\\times T$ matrix we are interested in with $\\mathbf{Y}=\\mathbf{L^*}+\\boldsymbol{\\varepsilon}$, where $\\boldsymbol{\\varepsilon}$ can be considered as measurement error. Athey et al. set up the methodological problem as estimating\n",
    "$$\n",
    "\\widehat{\\mathbf{L}}=\\mathrm{arg min}_{\\mathbf{L}}\\left\\{\\frac{1}{\\left|\\mathcal{O}\\right|}\\left\\| \\mathbf{P}_{\\mathcal{O}}(\\mathbf{Y}-\\mathbf{L}) \\right\\|_{F}^2 + \\lambda \\left\\| \\mathbf{L} \\right\\|_{*} \\right\\}.\n",
    "$$\n",
    "\n",
    "- The set of _non-missing_ elements is defined by all $(i,t)\\in\\mathcal{O}$, while the set of _missing_ elements by  $(i,t)\\in\\mathcal{M}$.\n",
    "- The term $\\mathbf{P}_{\\mathcal{O}}(\\mathbf{A})$ is an operator that sets to $0$ all the elements in matrix $\\mathbf{A}$ which do not belong to the set of matrix elements $\\mathcal{O}$. In other words, we just keep the non-missing elements $\\mathcal{O}$, but we replace the missing elements with 0's. $\\mathbf{P}_{\\mathcal{O}}^{\\bot}(\\mathbf{A})$ would do the opposite.\n",
    "- The term $\\left\\| \\cdot \\right\\|_{F}^2$ is the Frobenius Norm: $\\left\\| \\mathbf{A} \\right\\|_{F}^2 = \\sum_{i,j}A_{i,j}^2=\\sum_k\\sigma_k(\\mathbf{A})^2$, where $\\sigma_i(\\mathbf{A})$ are the singular values of the matrix (given by a Singular Value Decomposition).\n",
    "- The term $\\left\\| \\cdot \\right\\|_{*}$ is the **Nuclear Norm** regularization of the minimization problem, given by $\\left\\| \\mathbf{A} \\right\\|_{*} = \\sum_k\\sigma_k(\\mathbf{A})$. Recall that the singular values are always non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization has a very important role in the problem, since we want $\\mathbf{L}$ to approximate the matrix $\\mathbf{Y}$, not make it equal, but only taking into account the information from the non-missing elements. The Nuclear Norm, from other matrix norms, is appropriate here because it makes the problem a convex optimization problem (see discussion in the paper, page. 14)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Athey et al. propose an iterative algorithm which may seem weird at first as a solution to the problem. But before we go there, notice the following:\n",
    "- In principle, for a complete matrix $\\mathbf{Y}$, it holds that $\\mathbf{Y} = \\mathbf{P}_{\\mathcal{O}}(\\mathbf{Y}) + \\mathbf{P}_{\\mathcal{O}}^{\\bot}(\\mathbf{Y})$.\n",
    "- However, we do not have the values for the opperation $\\mathbf{P}_{\\mathcal{O}}^{\\bot}(\\mathbf{Y})$ to work. \n",
    "- Assuming that we had some $\\mathbf{L}_k$ that approximates the missing values of $\\mathbf{Y}$ we could have  $\\mathbf{P}_{\\mathcal{O}}^{\\bot}(\\mathbf{L}_k)$ in lieu.\n",
    "- But we want to use the information in $\\mathbf{P}_{\\mathcal{O}}(\\mathbf{Y})$ to generate the approximation $\\mathbf{L}_k$. The way to do this is by the approximation using the singular value decomposition. This is carried by the function $\\mathrm{shrink}_\\lambda(\\mathbf{A})$. \n",
    "- To be clear, $\\mathrm{shrink}_\\lambda(\\mathbf{A})$ is simply a function that returns an approximated version of the matrix $\\mathbf{A}$ using a singular value decomposition by taking only the singular values larger or equal than $\\lambda$. In other words, if $\\mathbf{A}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^T$\n",
    "$$\n",
    "\\mathrm{shrink}_\\lambda(\\mathbf{A}) = \\mathbf{U}\\boldsymbol{\\Sigma}_{\\mathrm{reduced}}\\mathbf{V}^T,\n",
    "$$\n",
    "where $\\boldsymbol{\\Sigma}_{\\mathrm{reduced}}$ is equal to $\\boldsymbol{\\Sigma}$ except that all diagonal elements $k$ less than $\\sigma_k(\\mathbf{A})<\\lambda$ have been set to zero.\n",
    "- Now, if we initialize $\\mathbf{L}_1$ with some value, we can have an approximation for $\\mathbf{Y}$ because $\\mathbf{L}_2 = \\mathrm{shrink}_\\lambda(\\mathbf{P}_{\\mathcal{O}}(\\mathbf{Y}) + \\mathbf{P}_{\\mathcal{O}}^{\\bot}(\\mathbf{L}_1))$.\n",
    "- Maybe if we repeat this many times, it will converge to some reasonable solution such that $\\widehat{\\mathbf{L}}=\\lim_{n\\rightarrow\\infty}\\mathbf{L}_n$.\n",
    "\n",
    "I suppose they realized of this by thinking backwards. In other words, the solution $\\widehat{\\mathbf{L}}$ should satisfy the relation, for a given value of $\\lambda$: \n",
    "$$\\widehat{\\mathbf{L}} = \\mathrm{shrink}_\\lambda(\\mathbf{P}_{\\mathcal{O}}(\\mathbf{Y}) + \\mathbf{P}_{\\mathcal{O}}^{\\bot}(\\widehat{\\mathbf{L}})).$$ This is what's behind the iterative estimation algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"Matrix-Completion with Nuclear Norm Minimization\" estimator (MCNNM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, they write it like:\n",
    "$$\n",
    "\\mathbf{L}_{k+1}(\\lambda, \\mathcal{O})=\\mathrm{shrink}_{\\frac{\\lambda |\\mathcal{O}| }{2}}\\left\\{\\mathbf{P}_{\\mathcal{O}}(\\mathbf{Y}) + \\mathbf{P}_{\\mathcal{O}}^{\\bot}(\\mathbf{L}_{k}(\\lambda, \\mathcal{O}))\\right\\}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximation, and as a consequence the estimate, will depend on the value of $\\lambda$. As a hyper-parameter, we will choose its value through cross-validation.\n",
    "\n",
    "Athey et al. propose to use some sort of $K$-fold cross-validation, and choose $K$ such that \n",
    "$$\\left|\\mathcal{O}_k\\right|/\\left|\\mathcal{O}\\right| = \\left|\\mathcal{O}\\right|/(NT).$$\n",
    "That is, such that the fraction of non-missing values in a given validation set $\\mathcal{O}_k$ is equal to the fraction of non-missing values in the original matrix. We will do this making use of Grid Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin with the P operators\n",
    "def PO_operator(Amat, setO):\n",
    "    \"\"\"\n",
    "    Set all elements that are not in setO to 0. It assumes that setO\n",
    "    comes from applying np.nonzero(A==condition) to a matrix.\n",
    "    \"\"\"\n",
    "    Anew = np.zeros_like(Amat)\n",
    "    Anew[setO] = Amat[setO]\n",
    "\n",
    "    return Anew\n",
    "\n",
    "def POcomp_operator(Amat, setO):\n",
    "    \"\"\"\n",
    "    The complement of PO_operator.\n",
    "    Set all elements that are in setO to 0. It assumes that setO\n",
    "    comes from applying np.nonzero(A==condition) to a matrix.\n",
    "    \"\"\"\n",
    "    Anew = np.copy(Amat)\n",
    "    Anew[setO] = 0\n",
    "\n",
    "    return Anew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets code the shrink operator\n",
    "def shrink(Amat, lamb=0, doprint=False):\n",
    "    \"\"\"\n",
    "    This generates a reduced version of A given by the singular value decomposition.\n",
    "    It only takes the singular above lamb.\n",
    "    \"\"\"\n",
    "    U, Sigma, VT = np.linalg.svd(Amat, full_matrices=False)\n",
    "    \n",
    "    if(doprint): print(Sigma)\n",
    "    \n",
    "    Sigma[Sigma < lamb] = 0\n",
    "    \n",
    "    return U@np.diag(Sigma)@VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss function\n",
    "def loss(Ymat, Lmat, setO, doprint=False):\n",
    "    Ocardinality = len(setO[0])\n",
    "    diffmat = Ymat - Lmat\n",
    "    if(doprint): print(diffmat)\n",
    "    outmat = PO_operator(diffmat, setO)**2\n",
    "    return (outmat.sum().sum()/Ocardinality)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"Matrix-Completion with Nuclear Norm Minimization\" estimator\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class MatrixCompletion_NNM(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This implements the iterative procedure to estimate L. \n",
    "    Since L is really the matrix Y, but with the missing values\n",
    "    imputed, we chose the Transformer 'Mixin'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    setOk : array-like\n",
    "        This comes, for example, from applying np.nonzero(A==condition) to a matrix\n",
    "    \n",
    "    missing_values : number, string, np.nan (default) or None\n",
    "        The placeholder for the missing values. All occurrences of\n",
    "        `missing_values` will be imputed.\n",
    "        \n",
    "    lamb : int (default=0)\n",
    "        This is the regularization parameter, which should be non-negative since\n",
    "        it is a minimization procedure.\n",
    "        \n",
    "    epsilon : float (default=0.001)\n",
    "        Desired accuracy of the estimation.\n",
    "    \n",
    "    max_iters : integer (default=100)\n",
    "    \n",
    "    doprint : boolean, optional (default=False)\n",
    "        Prints different results of the estimation steps.\n",
    "        \n",
    "    printbatch : integer, optional if doprint==True (default=10)\n",
    "        After how many iterations to print the loss function.\n",
    "        \n",
    "    copy : boolean, optional, default True\n",
    "        Set to False to perform inplace row normalization and avoid a\n",
    "        copy (if the input is already a numpy array).\n",
    "    \n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    Lest_ : array-like, shape (N, T)\n",
    "        This is the estimate of L.\n",
    "    \n",
    "    loss_ : float\n",
    "        The root-square of the mean square error of the observed elements.\n",
    "        \n",
    "    iters_ : int\n",
    "        Number of iterations it took the algorithm to get the desired\n",
    "        precision given by the parameter 'epsilon'.\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> data = np.array([[1,2,np.nan, 0],[np.nan,2,2,1],[3,0,1,3],[0,0,2,np.nan]])\n",
    "    >>> observedset = np.nonzero(~np.isnan(data))\n",
    "    >>> my_mcnnm = MatrixCompletion_NNM(setOk=observedset, lamb=2.5, epsilon=10**(-6), doprint=False)\n",
    "    >>> print(data)\n",
    "    [[  1.   2.  nan   0.]\n",
    "    [ nan   2.   2.   1.]\n",
    "    [  3.   0.   1.   3.]\n",
    "    [  0.   0.   2.  nan]]\n",
    "    >>> print(my_mcnnm.fit(data))\n",
    "    MatrixCompletion_NNM(copy=True, doprint=False, epsilon=1e-06, lamb=2.5,\n",
    "               max_iters=100, printbatch=10,\n",
    "               setOk=(array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3], dtype=int64), \n",
    "               array([0, 1, 3, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2], dtype=int64)))\n",
    "    >>> print(my_mcnnm.transform(data))\n",
    "    [[ 0.98082397  2.00915981  3.49022202  0.01813745]\n",
    "     [ 1.55601129  1.33063721  2.38547663  0.97025289]\n",
    "     [ 3.00567286  0.33134019  0.80761928  3.00948113]\n",
    "     [ 0.0419737   0.87320627  1.48553996 -0.39839039]]\n",
    "    >>> print(my_mcnnm.fit_transform(data))\n",
    "    [[ 0.98082397  2.00915981  3.49022202  0.01813745]\n",
    "     [ 1.55601129  1.33063721  2.38547663  0.97025289]\n",
    "     [ 3.00567286  0.33134019  0.80761928  3.00948113]\n",
    "     [ 0.0419737   0.87320627  1.48553996 -0.39839039]]\n",
    "    >>> print(my_mcnnm.transform([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]]))\n",
    "    [[ 0.98082397  2.00915981  3.49022202  0.01813745]\n",
    "     [ 1.55601129  1.33063721  2.38547663  0.97025289]\n",
    "     [ 3.00567286  0.33134019  0.80761928  3.00948113]\n",
    "     [ 0.0419737   0.87320627  1.48553996 -0.39839039]]\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, setOk=None, #missing_values=np.nan, \n",
    "                 lamb=0, epsilon=0.001, max_iters=100, \n",
    "                 doprint=False, printbatch=10, copy=True):\n",
    "        self.setOk = setOk \n",
    "        #self.missing_values = missing_values \n",
    "        self.lamb = lamb \n",
    "        self.epsilon = epsilon \n",
    "        self.max_iters = max_iters \n",
    "        self.doprint = doprint \n",
    "        self.printbatch = printbatch\n",
    "        self.copy = copy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the estimator to the matrix X (which is \n",
    "        really the matrix Y in Athey et al.'s paper).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : Ignored\n",
    "            There is no need of a target in a transformer, yet the pipeline API\n",
    "            requires this parameter.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        \n",
    "        # First, I should check that the missing values are right\n",
    "        #Ynew = np.zeros_like(self.Ymat)\n",
    "        #Ynew = self.missing_values\n",
    "        #Ynew[self.setOk] = self.Ymat[self.setOk]\n",
    "\n",
    "        assert (len(self.setOk) > 0), \"setOk should be a non-empty array\"\n",
    "        assert (self.lamb > 0), \"lamb is the lambda parameter which should be larger than zero\"\n",
    "\n",
    "        N, T = X.shape\n",
    "\n",
    "        # Initialize L to the observed (non-missing) values of Y given by the set setOk\n",
    "        Lprev = PO_operator(X, self.setOk)\n",
    "\n",
    "        # Initialization of error with a highvalue and the iteration\n",
    "        error = N*T*10**3\n",
    "        iteration = 0\n",
    "\n",
    "        while((error > self.epsilon) and (iteration < self.max_iters)):\n",
    "            Lnext = shrink(PO_operator(X, self.setOk) + \n",
    "                           POcomp_operator(Lprev, self.setOk), lamb = self.lamb)\n",
    "\n",
    "            # Updating values\n",
    "            Lprev = Lnext.copy()\n",
    "            error = loss(X, Lprev, self.setOk)\n",
    "            iteration = iteration + 1\n",
    "\n",
    "            if(self.doprint and (iteration%self.printbatch==0 or iteration==1)):\n",
    "                print(\"Iteration {}\\t Current loss: {}\".format(iteration, error))\n",
    "\n",
    "        if(self.doprint):\n",
    "            print(\"\")\n",
    "            print(\"Final values:\")\n",
    "            print(\"Iteration {}\\t Current loss: {}\".format(iteration, error))\n",
    "            print(\"\")\n",
    "            print(X)\n",
    "            print(np.round(Lnext, 2))\n",
    "        \n",
    "        self.iters_ = iteration\n",
    "        self.loss_ = error\n",
    "        self.Lest_ = Lnext\n",
    "        \n",
    "        # Return the transformer\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\" \n",
    "        Actually returning the estimated matrix, in which we have \n",
    "        imputed the missing values of X (matrix Y).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse-matrix}, shape (N, T)\n",
    "            The input data to complete.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X_transformed : array, shape (N, T)\n",
    "            The array the completed matrix.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            getattr(self, \"Lest_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must estimate the model before transforming the data!\")\n",
    "        \n",
    "        return self.Lest_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.   2.  nan   0.]\n",
      " [ nan   2.   2.   1.]\n",
      " [  3.   0.   1.   3.]\n",
      " [  0.   0.   2.  nan]]\n",
      "MatrixCompletion_NNM(copy=True, doprint=False, epsilon=1e-06, lamb=2.5,\n",
      "           max_iters=100, printbatch=10,\n",
      "           setOk=(array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3], dtype=int64), array([0, 1, 3, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2], dtype=int64)))\n",
      "[[ 0.98082397  2.00915981  3.49022202  0.01813745]\n",
      " [ 1.55601129  1.33063721  2.38547663  0.97025289]\n",
      " [ 3.00567286  0.33134019  0.80761928  3.00948113]\n",
      " [ 0.0419737   0.87320627  1.48553996 -0.39839039]]\n",
      "[[ 0.98082397  2.00915981  3.49022202  0.01813745]\n",
      " [ 1.55601129  1.33063721  2.38547663  0.97025289]\n",
      " [ 3.00567286  0.33134019  0.80761928  3.00948113]\n",
      " [ 0.0419737   0.87320627  1.48553996 -0.39839039]]\n",
      "[[ 0.98082397  2.00915981  3.49022202  0.01813745]\n",
      " [ 1.55601129  1.33063721  2.38547663  0.97025289]\n",
      " [ 3.00567286  0.33134019  0.80761928  3.00948113]\n",
      " [ 0.0419737   0.87320627  1.48553996 -0.39839039]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[1,2,np.nan, 0],[np.nan,2,2,1],[3,0,1,3],[0,0,2,np.nan]])\n",
    "observedset = np.nonzero(~np.isnan(data))\n",
    "\n",
    "my_mcnnm = MatrixCompletion_NNM(setOk=observedset, lamb=2.5, epsilon=10**(-6), doprint=False)\n",
    "\n",
    "#print(my_mcnnm.transform(data))\n",
    "\n",
    "print(data)\n",
    "\n",
    "print(my_mcnnm.fit(data))\n",
    "\n",
    "print(my_mcnnm.transform(data))\n",
    "\n",
    "print(my_mcnnm.fit_transform(data))\n",
    "\n",
    "print(my_mcnnm.transform([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 ..., 1 1 2]\n",
      " [2 1 1 ..., 1 3 2]\n",
      " [1 1 1 ..., 1 1 1]\n",
      " ..., \n",
      " [1 1 2 ..., 2 1 1]\n",
      " [1 1 1 ..., 1 3 1]\n",
      " [1 2 1 ..., 1 1 1]]\n",
      "[0.5, 0.70710678118654757, 1.0, 1.4142135623730951, 2.0, 2.8284271247461903, 4.0, 5.6568542494923806, 8.0, 11.313708498984761]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agomez\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\agomez\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "Y = np.random.logseries(0.3, size = (30, 40))\n",
    "lambda_values = [0.5*2**n for n in np.arange(0, 5, 0.5)]\n",
    "\n",
    "print(Y)\n",
    "print(lambda_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, cv=5, n_jobs=1, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "def do_CV()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to say I was slightly disappointed by the application of the method to ''real'' data. Athey et al. do not really apply to actual data, with actual treated units at specific times. In other words, in the paper they use a matrix where there are no missing entries (i.e., there is no treatment), and they choose a subset of units and some randomly selected times to assign a hypothetical treatment (that is kept following the initial period). After ''pretending'' the data is missing for those units at those times, the aim is the reconstruct the missing entries in the matrix.\n",
    "\n",
    "They compare the reconstruction error for different methods, such as Difference-in-differences, and different versions of synthetic control approaches, which Athey et al. show can all be interpreted under the unifying framework of matrix factorization methods:\n",
    "$$\n",
    "    \\widehat{\\mathbf{L}}^{\\mathrm{est}}=\\mathbf{A}^{\\mathrm{est}}{\\mathbf{B}^{\\mathrm{est}}}^T,\n",
    "$$\n",
    "for some $\\mathbf{A}^{\\mathrm{est}}$ and $\\mathbf{B}^{\\mathrm{est}}$ which are solutions to optimization procedures of the form\n",
    "$$\n",
    "    \\mathrm{arg min}_{(\\mathbf{A},\\mathbf{B})}\\left\\{\\frac{1}{\\left|\\mathcal{O}\\right|}\\left\\| \\mathbf{P}_{\\mathcal{O}}(\\mathbf{Y}-\\mathbf{A}\\mathbf{B}^T) \\right\\|_{F}^2 + \\mathrm{regularization~~on~~} \\mathbf{A}\\mathrm{~~and~~}\\mathbf{B}\\right\\},\n",
    "$$\n",
    "plus some other restrictions on $\\mathbf{A}$ and $\\mathbf{B}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because they artificially create the ''treatment'', we are going to implement their two options:\n",
    "1. **Simultaneous adoption**: where $N_t$ units adopt simultaneously the treatment in period $t_0+1$, while the other units never adopt the treatment.\n",
    "2. **Staggered adoption**: where $N_t$ units adopt the treatment, but each does so in randomly chosen times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simul_adopt(Ymat, treated_units, t0):\n",
    "    \"\"\"\n",
    "    The function assumes that t0 are integer values that go\n",
    "    from 1 to T, where T is the wide length of the matrix Ymat.\n",
    "    \"\"\"\n",
    "    Ynew = Ymat.copy()\n",
    "    Ynew[treated_units, t0:] = np.nan\n",
    "    return Ynew\n",
    "\n",
    "def stag_adopt(Ymat, treated_units, t0):\n",
    "    \"\"\"\n",
    "    Here we assume that units get treated at times \n",
    "    t after t0, chosen uniformly at random from the \n",
    "    remaining times.\n",
    "    \"\"\"\n",
    "    N, T = Ymat.shape\n",
    "    ts = np.random.choice(np.arange(t0,T), size = len(treated_units))\n",
    "    Ynew = Ymat.copy()\n",
    "    for i, unit in enumerate(treated_units):\n",
    "        Ynew[unit,ts[i]:] = np.nan\n",
    "    return Ynew\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simul_adopt(Lest, [0,2], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stag_adopt(Lest, [0,2], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Abadie-Diamond-Hainmueller California Smoking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_FROM_URL = \"https://raw.githubusercontent.com/susanathey/MCPanel/master/tests/examples_from_paper/california/\"\n",
    "\n",
    "X = pd.read_csv(PATH_FROM_URL + 'smok_covariates.csv',header=None).values\n",
    "Y = pd.read_csv(PATH_FROM_URL + \"smok_outcome.csv\", header=None).values.T\n",
    "treat = pd.read_csv(PATH_FROM_URL + 'smok_treatment.csv',header=None).values.T\n",
    "years = np.arange(1970,2001)\n",
    "\n",
    "## First row (treated unit)\n",
    "CA_y = Y[1,:]\n",
    "\n",
    "## Working with the rest of matrix (i.e., dropping first row)\n",
    "treat = treat[1:,:] \n",
    "Y = Y[1:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that np.printoptions() is only available in version 1.15 and later of Numpy.\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(X)\n",
    "    print(X.shape)\n",
    "    print(Y)\n",
    "    print(Y.shape)\n",
    "    print(treat)\n",
    "    print(treat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Y, cmap=plt.cm.gray);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
